---
title: PredviÄ‘anje rezultata Formule 1
---

# PredviÄ‘anje rezultata trke Formule 1

Polaznici: Darija VasiljeviÄ‡, Marina StojilkoviÄ‡  
Mentori: Andrej BantuliÄ‡, Milica Gojak i Marija NedeljkoviÄ‡

# Apstrakt

U ovom radu radu razvijen je sistem za predviÄ‘anje konaÄnog redosleda vozaÄa u trkama Formule 1 koristeÄ‡i javno dostupne podatke iz perioda 2018-2024 godine. UporeÄ‘ene su performanse statistiÄkih modela (linearna regresija, SVM, Naivni Bajes) i savremenih algoritama maÅ¡inskog uÄenja (duboke neuronske mreÅ¾e i XGBoost) koji su trenirani nag malim skupom podataka. Razmatrana su dva pristupa:  listwise (predviÄ‘anje cele liste) i pairwise (predviÄ‘anje vozaÄa na boljoj poziciji iz parova). Evaluacija modela pokazuje da pairwise pristupi, naroÄito XGBoost treniran nad parovima, postiÅ¾u najbolje performanse: RMSE \= 1.58, NDCG \= 0.987, MRR \= 0.89, Kendall Tau \= 0.86 i Spearman \= 0.92, Å¡to ukazuje na precizno rangiranje vozaÄa i visoku taÄnost predviÄ‘anja pobednika. Rad demonstrira da kombinacija obrade karakteristika i modernih modela moÅ¾e znaÄajno unaprediti predikciju rezultata u dinamiÄnom sportu poput Formule 1\.

# Abstract

# 1. Uvod

Formula 1 je sport koji se u velikoj meri oslanja na analizu podataka. Informacije koje timovi prikupljaju u toku trkaÄkih vikenda su veoma znaÄajne za osmiÅ¡ljanje adekvatne strategije za trku, podeÅ¡avanje bolida, otkrivanje potencijalnih problema i unapreÄ‘enje bolida. Za vreme trke i pripremnih treninga prikupljaju se podaci sa preko 250 senzora na bolidu. U toku jednog kruga prikupi se oko 30MB podataka. U Mercedesu tvrde da tokom jednog vikenada prikupe preko jednog terabajta podataka.   
VeÄ‡ina prethodnih radova koji se bave predviÄ‘anjem rerzultata trke Formule 1 nemaju cilj da rangiraju sve vozaÄe. Na primer, rad \[1\] bavi se klasifikacijom vozaÄa na kraju trke u 4 klase. Klasa 0 predstavlja pozicije od 11\. do 20, klasa 1 od 7\. do 10\. mesta, klasa 2 ukljuÄuje 4, 5\. i 6\. poziciju, dok klasa 3 obuhvata prva 3 mesta. Ovakav pristup ima taÄnost 65% kada se koristi duboka neuralna mreÅ¾a (DNN) mreÅ¾a. Umesto dobijanja cele rang-liste dobija se klasifikacija u 4 klase, Å¡to znaÄajno ograniÄava praktiÄnu primenu. Model ne moÅ¾e steÄ‡i adekvatnu sliku o performansama vozaÄa, niti pruÅ¾iti uvid u moguÄ‡i tok trke ili se koristiti za kompleksnija predviÄ‘anja.   
Cilj ovog projekta je konstruisanje sistema koji, koristeÄ‡i javno dostupne podatke, predviÄ‘a poredak vozaÄa na kraju trke.

# 2.Metod

## 2.1.Podaci i karakteristike 

Podaci o vozaÄima, stazama, konstruktorima, rezultatima trka, kvalifikacija, sprintova, kao i poredak vozaÄa u svakom krugu trke i njihova vremena su preuzeti sa Jolpica F1 API-ja. Podaci o vremenskim uslovima su dobijeni pomoÄ‡u Fast F1 biblioteke u Python-u. Inicijalno bilo je predviÄ‘eno da se koriste podaci od poÄetka hibridne ere (2014), jer je tada doÅ¡lo do promene pravila i sama dinamika sporta se dosta promenila. MeÄ‘utim, podaci o vremenu iz Fast F1 biblioteke dostupni su tek od 2018\. Stoga, odluÄeno je da skup podataka sadrÅ¾i trke od 2018\. do 2024\. godine, ukupno 7 sezona. Od koriÅ¡Ä‡enih karakteristika, 18 je direktno preuzeto iz prethodno navedenih izvora dok je 10 izvedeno na osnovu postojeÄ‡ih podataka. Karakteristike koje se koriste u ovom radu prikazane su u tabeli 1\.

| Grupa | Karakteristike | ObjaÅ¡njenje |
| :---- | :---- | :---- |
| **OpÅ¡te informacije** | sezona, runda, naziv trke, sprint vikend | Osnovne informacije o Velikoj nagradi. |
| **VozaÄ i tim** | broj vozaÄa, identifikator vozaÄa, tim | Identifikacija vozaÄa i tima. |
| **Performanse** | kvalifikaciona pozicija, startna pozicija | Rezultati vozaÄa tokom vikenda pre trke. |
| **Staza** | naziv staze, duÅ¾ina kruga, broj krugova, broj krivina | Karakteristike staze. |
| **Vremenski uslovi** | temperatura vazduha, vlaÅ¾nost, pritisak, temperatura staze, brzina vetra, pravac vetra, kiÅ¡a | MeteoroloÅ¡ki uslovi tokom trke. |
| **Istorijske i izvedene** | vreme pobednika proÅ¡le godine, proseÄna pozicija (poslednjih 5 trka), odustajanja (poslednjih 5 trka), dobijene/izgubljene pozicije (poslednjih 5 trka), prosek trajanja zamene guma po timu (5 trka), proseÄno vreme koje se izgubi prilikom zamene guma, broj stajanja ne stazi u prethodnoj godini, broj preticanja po vozaÄu na istoj stazi u proÅ¡loj sezoni, najbolje kvalifikaciono vreme vozaÄa | Karakteristike izvedene iz istorijskih i agregiranih podataka. |

*Tabela 1*: Karakteristike koriÅ¡Ä‡ene prilikom treniranja modela 

Na slici 1 prikazana je matrica korelacije karakteristika definisanih u tabeli 1\. VeÄ‡ina karakteristika nema izraÅ¾enu meÄ‘usodnu korelaciju. Jaka negativna korelacija prisutna je kod karakteristika koje opisuju broj krugova i duÅ¾inu staze jer duÅ¾ina Velike nagrade iznosi najmanje 300km, sa izuzetkom Velike nagrade Monaka. Jaka pozitivna korelacija uoÄava se  izmeÄ‘u poÄetne pozicije vozaÄa i rezultata na kraju trke.  
 ![*Slika 1*: Matrica korelacije koriÅ¡Ä‡enih karakteristika](images/zbornik/2025/formula-1/matrica-korelacije.png)
 

## 2.2.Metrike

### 2.2.1.NDCG â€“ *Normalized Discounted Cumulative Gain*

NDCG je mera kvaliteta rangiranja koja opisuje koliko uspeÅ¡no algoritam rangira stavke prema njihovoj relevantnosti.  Osnovna ideja iza ove metrike je da su greÅ¡ke na viÅ¡im pozicijama (tj. na vrhu liste) znaÄajnije nego greÅ¡ke na niÅ¾im pozicijama. Ukoliko model rangira manje relevantne stavke visoko, metrika Ä‡e kazniti takvo ponaÅ¡anje viÅ¡e nego ako su te greÅ¡ke pri dnu.

Ova vrednost se raÄuna kroz tri etape:

1. Discounted Cumulative Gain (DCG) se raÄuna kao:

   DCGk \= i \= 1krelilog2(i \+ 1\)

   Gde je:

* reliâ€” relevantnost stavke na poziciji *i* (u ovom eksperimentu poziciji 1 odgovara vrednost 20, drugoj 19, a poslednjoj relevantnost   
* *k* â€” broj pozicija koje se uzimaju u obzir (u ovom eksperimentu uzima se vrednost 10, jer prvih 10 vozaÄa dobija poene).


2. *Ideal* DCG (IDCG) predstavlja maksimalni moguÄ‡i DCG za date relevantnosti, tj. vrednost DCG kada su stavke savrÅ¡eno rangirane po relevantnosti.  
3. *Normalized* DCG (NDCG) se definiÅ¡e kao odnos ostvarenog DCG i idealnog DCG:

   NDCGk=DCGkIDCGk

Vrednosti ove metrike su u intervalu \[0, 1\], gde 1 oznaÄava savrÅ¡eno rangiranje, dok vrednosti bliÅ¾e 0 oznaÄavaju loÅ¡e performanse algoritma rangiranja.

### 2.2.2.*Kendall's ğœ* 

*Kendall's* ğœ je statistiÄka mera koja procenjuje sliÄnost izmeÄ‘u dva rangiranja. Zasniva se na broju saglasnih (konkordantnih) i nesaglasnih (diskordantnih) parova u dva poreÄ‘enja. Za niz od n elemenata, Kendall's ğœ se raÄuna kao:

 ğœ = 2(C-D)n(n \- 1),  
gde su:

*    *C* â€” broj konkordantnih parova,  
*    *D* â€” broj diskordantnih parova.

Vrednosti ğœ se kreÄ‡u u opsegu \[-1, 1\], gde 1 oznaÄava savrÅ¡eno slaganje rangova, 0 odsustvo korelacije, a \-1 potpuno obrnut redosled.

### 2.2.3.Spearmanov rang korelacije

Spearmanova korelacija meri koliko su dva rangiranja sliÄna. Umesto da gleda stvarne vrednosti, posmatra samo redosled elemenata.  
Za niz od *n* elemenata, prvo se izraÄunaju razlike izmeÄ‘u rangova svakog elementa u dve liste, oznaÄene kao di.  
Speranov rang korelacije se definiÅ¡e kao:  
 â´ = 1 \- 6 di2n (n2-1)

Vrednosti *â´* se kreÄ‡u od \-1 (obrnuti rangovi) do 1 (savrÅ¡eno slaganje rangova), dok oznaÄava *â´ \= 0* odsustvo monotone veze.

### 2.2.4. *Root Mean Squared Error*

RMSE je standardna mera koja pokazuje proseÄnu veliÄinu greÅ¡ke izmeÄ‘u stvarnih i predviÄ‘enih vrednosti. IzraÄunava se kao kvadratni koren proseÄne kvadratne greÅ¡ke:

$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$
 
gde su:

*    *y* â€” stvarna vrednost,  
*     Å· â€” predviÄ‘ena vrednost,  
*    *n* â€” broj vozaÄa.

Manja vrednost RMSE znaÄi da su predviÄ‘anja bliÅ¾a stvarnim vrednostima.

## 2.3.StatistiÄke metode 

### 2.3.1. Linearna regresija

Linearna regresija predstavlja jednu od najosnovnijih statistiÄkih i maÅ¡inskih metoda za modelovanje zavisnosti izmeÄ‘u jedne zavisne promenljive (target) i jedne ili viÅ¡e nezavisnih promenljivih (feature). SuÅ¡tina linearne regresije ogleda se u pretpostavci da postoji linearna veza izmeÄ‘u ulaznih karakteristika i izlazne vrednosti, koja se moÅ¾e opisati linearnom funkcijom oblika:

y \= Î²â‚€ \+ Î²â‚xâ‚ \+ Î²â‚‚xâ‚‚ \+ â€¦ \+ Î²nxn 

gde su:

* y \- predikcija zavisne promenljive  
* xâ‚, xâ‚‚, â€¦, â€‹ xn \- nezavisne promenljive  
* Î²â‚€, Î²â‚, Î²â‚‚, â€¦, Î²n \- parametri modela koji odreÄ‘uju znaÄaj pojedinih faktora

U sluÄaju predviÄ‘anja rezultata trke Formule 1, linearna regresija se moÅ¾e koristiti u okviru pairwise pristupa, gde se vrÅ¡e poreÄ‘enja izmeÄ‘u parova vozaÄa. Za svaki par vozaÄa (i,j) formira se ulazni vektor razlika njihovih karakteristika, a model donosi odluku:

fáµ¢â±¼ \= Î²â‚€ \+ Î²â‚(xáµ¢â‚-xâ±¼â‚) \+ Î²â‚‚(xáµ¢â‚‚âˆ’xâ±¼â‚‚) \+ â€¦ \+ Î²â‚™(xáµ¢â‚™âˆ’xâ±¼â‚™)

Na osnovu ove vrednosti donosi se binarna odluka:

* 1, ako vozaÄ i zavrÅ¡ava ispred vozaÄa j  
* 0, ako vozaÄ j zavrÅ¡ava ispred vozaÄa i


Serijom ovakvih parnih poreÄ‘enja izmeÄ‘u svih vozaÄa u jednoj trci formira se konaÄna rang lista.

### 2.3.2. Support Vector Machine SVM

Support Vector Machine (SVM) predstavlja jednu od osnovnih metoda nadgledanog uÄenja koja se koristi za klasifikaciju i regresiju. SVM uÄi hiper-ravan koji najbolje razdvaja klase u prostoru karakteristika, maksimizujuÄ‡i marginu izmeÄ‘u podataka iz razliÄitih klasa.

U sluÄaju predviÄ‘anja rezultata trke Formule 1, kao i kod linearne regresije, SVM se takoÄ‘e moÅ¾e koristiti u okviru pairwise pristupa, gde se vrÅ¡e poreÄ‘enja izmeÄ‘u parova vozaÄa. Za svaki par vozaÄa (i,j) formira se ulazni vektor razlika njihovih karakteristika, a model donosi odluku:

fáµ¢â±¼ \= w Â· (xáµ¢ \- xâ±¼) \+ b

gde su:

* w  â€“ vektor teÅ¾ina modela  
* b \- slobodni Älan  
* xáµ¢, xâ±¼ \- vektori karakteristika vozaÄa *i* i *j*

Na osnovu ove vrednosti donosi se binarna odluka, 1 ako vozaÄ i zavrÅ¡ava ispred vozaÄa j ili 0, ako vozaÄ j zavrÅ¡ava ispred vozaÄa i.

### *2.3.3.* Naivini Bajes sa Laplasovim zaglaÄ‘ivanjem

Naivni Bajes je linearni probabilistiÄki klasifikator koji se zasniva na Bajesovoj formuli verovatnoÄ‡e hipoteze. Bajesova formula se zasniva na pretpostavci da sluÄajni dogaÄ‘aji H1, H2, ..., Hn Äine potpun sistem hipoteza, to jestd da predstavljaju ceo prostor dogaÄ‘aja i meÄ‘usobno su disjunktni. Ako je *A* dogaÄ‘aj za koji vaÅ¾i P(A) \> 0, tada se verovatnoÄ‡a da je hipoteza Hi dovela do realizacije dogaÄ‘aja *A* raÄuna po formuli:

P(Hi|A) \=P(Hi)P(A|Hi)P(A) ,

gde je:

* P(A) verovatnoÄ‡a da se odigrao dogaÄ‘aj A  
* P(Hi) verovatnÄ‡a hipoteze Hi  
* P(Aâˆ£Hi) uslovna verovatnoÄ‡a dogaÄ‘aja *A* pod uslovom Hi  
* P(Hi|A) verovatnoÄ‡a da je hipoteza Hi dovela do realizacija dogaÄ‘aja *A*

U ovom pristupu 70% podataka (103 trke) koriÅ¡Ä‡eno je za trening, a 30% (45 trka) za testiranje. Kako je ova podela izvrÅ¡ena hronoloÅ¡ki, neki vozaÄi, staze i timovi su nepoznati modelu. Da bi se ovaj probem reÅ¡io, koriÅ¡Ä‡eno je Laplasovo zagraÄ‘ivanje (*Laplace smoothening*) koje je predstavljeno formulom:  
P(Hi|A) \=P(Hi)P(A|Hi) \+ Î±P(A) \+ |V|,  
gde je:

* *Î±* \- korektivni faktor (u projektu iznosi 1\)  
* *|V|* \- broj svih moguÄ‡ih karakteristika (tim, vozaÄ, vremenske prilike, stazaâ€¦)


## 2.4.Duboka neuralna mreÅ¾a 

Duboke neuronske mreÅ¾e (DNN) predstavljaju skup viÅ¡e slojeva povezanih neurona koji omoguÄ‡avaju modelovanje sloÅ¾enih nelinearnih zavisnosti izmeÄ‘u ulaznih podataka i izlaza. One su posebno pogodne za zadatke gde interakcije izmeÄ‘u karakteristika nisu trivijalne i gde jednostavni linearni modeli ne mogu da uhvate skrivene obrasce u podacima.

Za preciznije predviÄ‘anje rezultata trka Formule 1 koristi se duboka neuronska mreÅ¾a koja modeluje zavisnosti izmeÄ‘u karakteristika vozaÄa i njihovog plasmana. KoriÅ¡Ä‡ena su dva pristupa. Prvi pristup je mreÅ¾a koja kao ulaz dobija svih 20 vozaÄa, a na izlazu daje celu rang listu. 

Pre nego Å¡to podatke prosledimo linearim slojevima, kategoriÄke vrednosti se prvo pretvaraju u numeriÄke pomoÄ‡u embedding slojeva. Embedding sloj mapira svaku kategoriju na vektor realnih brojeva, omoguÄ‡avajuÄ‡i linearnoj mreÅ¾i da ih analizira i uÄi odnose izmeÄ‘u razliÄitih kategorija. Kodiranje se radi pomoÄ‡u factorize funkcije iz biblioteke pandas u Python-u, koja svakoj jedinstvenoj kategoriji dodeljuje jedinstveni ceo broj, Äime se kategoriÄke vrednosti lako mogu proslediti embedding sloju.

Ova mreÅ¾a sadrÅ¾i viÅ¡e linearnih slojeva sa LeakyReLU aktivacijom, batch normalizacijom i dropout regularizacijom. Na slici 2 prikazana je precizna arhitektura mreÅ¾e.

![*Slika 2*: Arhitektura duboke neuralne mreÅ¾e](images\zbornik\2025\formula-1\dnn20.png)

Drugi pristup je mreÅ¾a koja kao ulaz dobija parove vozaÄa, a na izlazu daje predikciju koji je od njih bolji. Ova mreÅ¾a sadrÅ¾i tri linearna sloja sa ReLU aktivacijom, batch normalizacijom i dropout regularizacijom. Na slici 3 prikazana je precizna arhitektura ove mreÅ¾e.

![*Slika 3*: Arhitektura duboke neuralne mreÅ¾e za par po par pristup](images\zbornik\2025\formula-1\dnn-par.png)

## 

## 

## 

## 

## 2.5. Extreme Gradient Boosting \- XGBoost

Gradijentno pojaÄavanje je tehnika maÅ¡inskog uÄenja koja kombinovanjem viÅ¡e slabih modela kreira jedan jak prediktivni model. Zadatak svakog od modela (sem prvog) je da ispravi greÅ¡ke prethodnog. Na kraju, sve male odluke se spajaju u jednu jaku i taÄnu predikciju. XGBoost predstavlja primenjivanje ove tehnike nad stablima odluÄivanja. PoÄinjemo od jednog slabog modela koji se naziva osnovni model. Potom se stabla dodaju jedno po jedno. Algoritam raÄuna reziduale, odstupanja, predikcije svakog stabla. Svi reziduali se kombinuju i na osnovu loss funkcije se ocenjuje model. Kako bi se minimizovala funkcija gubitka, koristi se gradijentni spust. Neki hiperparametri (parametri Äija se vrednost ne menja u toku treninga) *XGBoost*\-a su stopa uÄenja ğœ‚, broj stabala, maksimalna dubina stabla i ğ›¾.  
Stopa uÄenja oznaÄava koliko Ä‡e se parametri modela menjati u jednoj iteraciji. Prevelika stopa uÄenja za posledicu moÅ¾e imati preskakanje minimuma funkcije gubitka. S druge strane, premala stopa uÄenja moÅ¾e dovesti do prespore konvergencije ili zaustavljanja na nekom lokalnom minimumu, ali ne na globalnom minimumu funkcije.   
Broj stabala predstavlja broj slabih modela koji Ä‡e se trenirati. Mali broj stabala donosi brÅ¾e treniranje, ali model moÅ¾e biti previÅ¡e jednostavan. Nasuprot tome, veliki broj stabala moÅ¾e da uÄi kompleksne obrasce, ali donosi i rizik od prenauÄenosti (overfitting). ObiÄno se ova vrednost ne podeÅ¡ava ruÄno, veÄ‡ se trening zaustavlja kada metrika na validacionom skupu prestane da se poboljÅ¡ava.   
Maksimalna dubina stabla je broj nivoa od korena do lista stabla. Kao i kod broja stabala, male vrednosti daju modele koji su sposobniji za generalizaciju, ali moÅ¾da neÄ‡e moÄ‡i da pronaÄ‘u kompleksnije obrasce, a velike vrednosti poveÄ‡avaju rizik od prenauÄenosti.   
Hiperparametar ğ›¾ kontroliÅ¡e da li Ä‡e se stablo deliti u nekom Ävoru. Ukoliko taj Ävor donosi veÄ‡e smanjenje loss funkcije od zadate vrednosti, stablo Ä‡e se granati, u suprotnom, na tom mestu ne postoji Ävor.  
Model *XGBoost* je implementiran kroz xgboost biblioteku u *Python*\-u. *XGBoost* je implematiran sa dve razliÄite ciljne funkcije NDCG i *pairwise.* Ciljna funkcija NDCG podrazumeva da model optimizuje loss koji aproksimira ovu metriku diferencijabilnom funkcijom. Rangiranje koriÅ¡Ä‡enjem ciljne funkcije *pairwise* podrazumeva da model predviÄ‘a koji vozaÄ Ä‡e ostvariti bolji plasman za svaki par vozaÄa.  
Za optimizaciju hiperparametara koriÅ¡Ä‡ena je biblioteka *Optuna*, koja primenjuje princip Bajesove optimizacije kroz algoritam *Tree-structured Parzen Estimator* (TPE). Umesto pretraÅ¾ivanja celog prostora parametara (kao Å¡to je to sluÄaj kod grid search-a) koji moÅ¾e imati hiljade kombinacija, *Optuna* bira vrednosti hiperparametara na osnovu prethodnih evaluacija modela, kako bi ubrzala pronalaÅ¾enje optimalne konfiguracije. Tokom procesa, *Optuna* trenira model sa razliÄitim skupovima hiperparametara, evaluira performanse na validacionom skupu (koriÅ¡Ä‡enjem NDCG metrike) i iterativno usmerava potragu ka kombinacijama koje imaju najveÄ‡i potencijal za dobar rezultat. Optimalne vrednosti nekih hiperparametara pronaÄ‘ene pomoÄ‡u ove biblioteke su prikazane u tabeli 2\. 

| Hiperparametar | XGBoost \- NDCG | XGBoost \- *pairwise* |
| :---- | :---- | :---- |
| stopa uÄenja ğœ‚ | 0,22887 | 0,29822 |
| maksimalna dubina stabla | 7 | 9 |
| parametar ğ›¾ | 0,21104 | 0,47453 |

*Tabela 2*: Neki od hiperparametara za XGBoost model

## 2.6. Problem hronoloÅ¡ke podele podataka

U maÅ¡inskom uÄenju vrlo Äesto podela podataka na set za treniranje, validaciju i test vrÅ¡i se nasumiÄno. MeÄ‘utim, u ovom radu situacija je drugaÄija. Formula 1 je sport koji se vrlo brzo menja. Promena pravila, razvoj bolida, sticanje iskustva samo su neki od faktora koji utiÄu na rezultate. Sve ove kategorije su promenljive u vremenu. Stoga, treba voditi raÄuna da se skup podataka hronoloÅ¡ki podeli. Najjednostavniji pristup bio bi da se prvih 70% posto trka koristi za treniranje narednih 10% za validaciju i preostalih 20% za testiranje. Ovakav pristup donosi brojna ograniÄenja. Pre svega, evolucija koja se deÅ¡ava kroz sezone obuhvaÄ‡ene validacijom i treningom. Sezone do 2020\. sa sobom nose i dominaciju Mercedesa. UÄeÄ‡i na ovakvim podacima, model moÅ¾e da nauÄi da je vozaÄ Mercedesa, ako ne na prvom mestu, onda gotovo sigurno na pobedniÄkom postolju. Ovo predstavlja naroÄito veliki problem, jer je na primer od 2021\. do sredine 2024\. Red Bull bio dominantan.

## 2.7. *Rolling window* tehnika i *XGBoost*

	Ovaj problem se moÅ¾e reÅ¡avati koriÅ¡Ä‡enjem *Rolling window* tehnike. Ona podrazumeva podelu skupa podataka na manje vremenske okvire. Postoje dva pristupa. U oba se poÄinje od najstarijih podataka, i oni koji slede predstavljaju podatke za testiranje.  Slika 4 prikazuje da prvi pristup podrazumeva pomeranje okvira za testiranje podataka ka novijim podacima. Drugi naÄin je da se okvir proÅ¡iruje, na primer poÄinjemo od cele 2018\. sezone, i postepeno proÅ¡irujemo da okvir ukljuÄuje sve do pete trke pred kraj 2024, a tih preostalih 5 koristimo kao test set. U ovom projektu tehnika *Rolling window* se kombinuje sa modelom *XGBoost*. VeliÄina prozora je konstantna i iznosi 30 trka, validacija se vrÅ¡ila na 2, a test na 4 trke. U svakoj iteraciji prozor se pomerao za 6 trka.  
![*Slika 4:* Prikaz *Rolling window* tehnike ](images\zbornik\2025\formula-1\roll.png)  


## 2.8. XGBoost sa pairwise treniranjem

	Pored ugraÄ‘enog pairwise pristupau XGBoost-u, isproban je joÅ¡ jedan pristup koji se zasniva na manuelnom treniranju nad parovima. Dok pomenut ugraÄ‘eni pairwise interno kreira sve parove unutar definisanih grupa i optimizuje rang direktno na nivou instanci, manuelni pairwise pristup zahteva da se parovi eksplicitno generiÅ¡u, pri Äemu se model trenira kao binarni klasifikator nad odnosom izmeÄ‘u Älanova para. Glavna razlika izmeÄ‘u ova dva pristupa leÅ¾i u naÄinu kreiranja parova i funkciji gubitka: XGBoost automatski upravlja parovima i koristi pairwise log loss, dok manuelni pristup koristi standardni binary log loss i zahteva dodatnu obradu kako bi se predikcije po parovima transformisale u konaÄni rang. 

# 3\. Rezultati

## 3.1 Performanse modela

| Model | *NDCG* | *Kendallâ€™s Ï„* | Spermanov rang korelacije | *RMSE* |
| :---- | :---- | :---- | :---- | :---- |
| Naivini Bajes sa Laplasovim zaglaÄ‘ivanjem | 0,6818 | 0,1329 | 0,1829 | 7,2450 |
| XGBoost \- pairwise | 0,9619 | 0,5753 | 0,7241 | 4,0755 |
| XGBoost \- NDCG | 0,9586 | 0,5694 | 0,7048 | 4,3400 |
| XGBoost \- Rolling window tehnika | \- | \- | \- | \- |

*Tabela 3*: Uporedni prikaz metrika za razliÄite modele

## 3.2. StatistiÄki modeli

Na slikama 5, 6 i 7 prikazani su rezultati predikcije plasmana vozaÄa u test skupu trka koristeÄ‡i tri pomenuta statistiÄka modela. Slika 5 prikazuje rezultate linearne regresije primenjene u pairwise pristupu. Slika 6 prikazuje rezultate dobijene koriÅ¡Ä‡enjem SVM-a, takoÄ‘e u pairwise pristupu. 

![*Slika 5*: Konfuziona matrica linearne regresije](images\zbornik\2025\formula-1\linreg.png)

![*Slika 6*: Konfuziona matrica SVM](images\zbornik\2025\formula-1\svm.png)


Model linearne regresije daje taÄnost 80,1%, a SVM 82,8%. Analizom ovih taÄnosti i konfuzionih matrica moÅ¾e se uoÄiti da oba modela uspeÅ¡no predviÄ‘aju relativni plasman u veÄ‡ini parova, ali retko uspevaju da precizno rekonstruiÅ¡u taÄan konaÄan plasman vozaÄa.

Na slici 7 je prikazana matrica konfuzije kada je koriÅ¡Ä‡en model Naivni Bajes sa Laplasovim zaglaÄ‘enjem. Njenom analizom moÅ¾e se uoÄiti da glavna dijagonala nije izraÅ¾ena. TakoÄ‘e, pobednik je ispravno predviÄ‘en svega 3 puta Å¡to ukazuje da model nema moguÄ‡nosti da generalizuje na novije podatke. Ovakvo ponaÅ¡anje je oÄekivano jer je Formula 1 veoma dinamiÄan sport i neophodni su kompleksniji modeli koji mogu da prate zavisnost plasmana od vremeskih uslova i konfiguracije staze, kao i evoluciju timova i vozaÄa. Vrednosti metrika iz tabele 3 ukazuju da ovaj model jako loÅ¡e rangira vozaÄe, u proseku sa greÅ¡kom veÄ‡om od 7 pozicija.  
   
![*Slika 7*: Matrica konfuzije za model naivni Bajes sa Laplasovim zaglaÄ‘enjem ](images\zbornik\2025\formula-1\bajes.png)

## 3.3. Duboka neuralna mreÅ¾a

Za treniranje mreÅ¾e nad svih 20 vozaÄa po trci koriÅ¡Ä‡en je SGD optimizator i ListNetLoss funkcija gubitka, specijalno dizajnirana za zadatke rangiranja. Na slici 8 prikazani su grafikoni gubitka i NDCG metrike kroz epohe.

![*Slika 8* : Gubitak i NDCG kroz epohe tokom treniranja duboke neuralne mreÅ¾e](images\zbornik\2025\formula-1\dnn-obican-trening.png)

Za treniranje druge mreÅ¾e nad parovima vozaÄa koriÅ¡Ä‡en je Adam optimizator i Binary Cross-entropy Loss With Logits funkcija gubitka. Ova funkcija gubitka kombinuje sigmoid aktivaciju i standardni binary cross-entropy, tako da model direktno uÄi verovatnoÄ‡u da je jedna instanca u paru rangirana viÅ¡e od druge. Ovakav pairwise pristup dubokoj neuralnoj mreÅ¾i dao je 74,5% taÄnosti. Na slici 9 prikazan je primer rangiranja jedne trke ovog modela.

![*Slika 9*: Primer rangiranja jedne trke duboke neuralne mreÅ¾e](images\zbornik\2025\formula-1\jedna-trka-primer.png)


## 3.4. XGBoost

![*Slika 10:*  Matica konfuzije za XGBoost sa pairwise cljnom funkcijom ](images\zbornik\2025\formula-1\xgb-pairwise.png)
![*Slika 11:*  Matica konfuzije za XGBoost sa NDCG cljnom funkcijom ](images\zbornik\2025\formula-1\xgb-ndcg.png)  


Na slici 11 prikazane su matrice konfuzije kada je XGBoost treniran sa ciljnom funkcijom NDCG i na slici 10 kada je koriÅ¡Ä‡ena ciljna funkcija *pairwise*. Na osnovu ovih konfuzionih matrica moÅ¾e se zakljuÄiti da *pairwise* pristup daje izraÅ¾eniju dijagonalu. Ovo je oÄekivano jer NDCG prioritizuje taÄno rangiranje prvih 10 vozaÄa, dok *pairwise* pristup bolje opisuje pojedinaÄne rezultate meÄ‘u vozaÄima. Oba modela su taÄno predvidela pobednika u 17 trka, dok sa *pairwise* ciljnom funkcijom model bolje predviÄ‘a i drugo mesto (10 pogoÄ‘enih nasuprot 8). Sve metrike prikazane u tabeli 3 ukazuju na znaÄajno bolje performanse ovih modela u poreÄ‘enju sa statistiÄkom analizom.

## 3.5. *Rolling window* tehnika i *XGBoost*

![*Slika 12:* Prikaz vrednosti metrika (NDCG, *Kendallâ€™s Ï„* i Spermanov rang korelacijie) na validaciji (levo) i vredvosti RMSE na validaciji (desno)](images\zbornik\2025\formula-1\metrika-roll.png)


Na slici 12 prikazane su vrednosti metrika na validaciji (NDCG, *Kendallâ€™s Ï„* i Spermanov rang korelacijie) i vrednosti RMSE. PrimeÄ‡uje se da se metrike ne podoljÅ¡avaju. NDCG veoma malo osciluje sa vrednostima izmeÄ‘u 0,9 i 1\. Kod vrednosti *Kendallâ€™s Ï„* i Spermanovog ranga korelacijie javljaju se veliki padovi. TakoÄ‘e, RMSE se ne spuÅ¡ta ispod 11 Å¡to ukazuje da model u proseku pravi toliku greÅ¡ku pri rangiranju vozaÄa. Kako nije primeÄ‡en rastuÄ‡i trend, odnosno opadajuÄ‡i za RMSE, za ovaj metod nije vrÅ¡eno testiranje. KoriÅ¡Ä‡enjem *Rolling window* tehnike prethodna stabla se ne modifikuju, veÄ‡ se na prethodna stabla dodaju se nova. Ova osobina XGBoost algoritma dovodi do prenauÄavanja i slabijih prediktivnih sposobnosti, zbog Äega metod nije dao zadovoljavajuÄ‡e rezultate. 

## 3.6. *XGBoost* sa *pairwise* treniranjem

Na slici 13 prikazana je konfuziona matrica *XGBoost*\-a treniranog na ruÄno formiranim parovima.   
![*Slika 13*: Konfuziona matrica *XGBoost*\-a treniranog na ruÄno formiranim parovima](images/zbornik/2025/formula-1/xgb-par.png)
Vrednosti metrika su:

* RMSE: 1.58 \- model u proseku greÅ¡i 1.58 mesta, Å¡to je relativno niska greÅ¡ka za probleme rangiranja 20 vozaÄa  
* NDCG: 0.987 \- model dobro rangira vrh liste  
* MRR: 0.89 \- model ima visoku Å¡ansu da pobednika predvidi visoko na rang listi  
* Kendall Tau: 0.86 \- postoji jaka pozitivna povezanost izmeÄ‘u predikcija i stvarnog ranga  
* Spearman: 0.92 \- model dobro uÄi relativni redosled instanci  

Na slici ispod prikazan je primer rangiranja jedne trke ovim modelom.

![*Slika 14*: Primer rangiranja jedne trke XGBoost-om treniranog nad parovima](images\zbornik\2025\formula-1\jedna-trka-xgb.png)



# Diskusija

StatistiÄki metodi nisu dali zadovoljavajuÄ‡e rezultate zbog svoje suviÅ¡e jednostavne prirode. Duboka neuralna mreÅ¾a (DNN) trenirana da predvidi celu rang-listu pokazala se neuspeÅ¡nom, pre svega zato Å¡to koriÅ¡Ä‡ena funkcija gubitka nije bila adekvatna za taj zadatak. XGBoost i DNN model treniran da predvidi koji je od dva vozaÄa bolji davaju znatno bolje rezultate. Sa druge strane, *Rolling Window* pristup zasnovan na XGBoost modelu pokazivao je izraÅ¾eno overfitovanje, Å¡to je ograniÄilo njegovu praktiÄnu primenu. Kao najefikasniji pristup pokazao se XGBoost sa treniranjem par po par.

## Predlozi poboljÅ¡anja

Metrike ukazuju na postojanje prostora za poboljÅ¡anje. MoguÄ‡a unapreÄ‘enja ukljuÄuju dodavanje novih karakteristika koje dodatno opisuju kontekst trke: istorijski rezultati vozaÄa na odreÄ‘enoj stazi, kao i performanse u specifiÄnim vremenskim uslovima (npr. pojedini vozaÄi postiÅ¾u znaÄajno bolje rezultate u kiÅ¡nim uslovima). Iako u ovom projektu stariji podaci nisu koriÅ¡Ä‡eni, oni bi mogli da daju kontekst modelu o funkcionisanju sporta, a na novijim podacima bi mogao da se *finetune*\-uje.  Prikupljanje novih podataka i inÅ¾enjering karakteristika bi omoguÄ‡ili i koriÅ¡Ä‡enje sloÅ¾enijih modela, poput mreÅ¾a za obradu vremenskih sekvenci, kao Å¡to su rekurentne neuralne mreÅ¾e, *Long-Short Term Memory* (LSTM) mreÅ¾e i transformeri koje bi znatno poboljÅ¡ale predviÄ‘anja.

## Literatura

\[1\] Kike FRANSSEN. COMPARISON OF NEURAL NETWORK ARCHITECTURES IN RA-  
CE PREDICTION Predicting the racing outcomes of the 2021 Formula 1 season. Masterâ€™s  
thesis, Tilburg University, 2022\.

\[2\] Octaviana-Alexandra Cheteles. Feature importance versus feature selection in predictive  
modeling for formula 1 race standings. B.S. thesis, University of Twente, 2024\.
